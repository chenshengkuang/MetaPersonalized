---
title: "mpersonalzied Package Tutorial"
author: "Chensheng Kuang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

First, load the package.
```{r load}
library(mpersonalized)
```

There is a function built up to display and test the package. We can supply a seed to the function so it can reproduce simulated data set that we want.

For example, we can first generate some data for the meta-analysis problem. 
```{r simulate data}
sim_data = simulated_dataset(n = 200, sim_seed = 123, problem = "meta-analysis")

names(sim_data)

B = sim_data$B

mainB = B[,1:12]
mainB

interactionB = B[,52:57]
interactionB

Xlist = sim_data$Xlist; Ylist = sim_data$Ylist; Trtlist = sim_data$Trtlist

str(Xlist)
str(Ylist)
str(Trtlist)

```
To generate test data for multiple outcomes problem, we only need to change the argument `problem` to be "multiple outcomes". 

The data format is slightly different, for example,
```{r multiple outcomes simulated}
sim_data = simulated_dataset(n = 200, sim_seed = 123, problem = "multiple outcomes")

names(sim_data)

X = sim_data$X; Ylist = sim_data$Ylist; Trt = sim_data$Trt

str(X)
str(Ylist)
str(Trt)
```

```{r simulate data for meta-analysis, echo = FALSE}
sim_data = simulated_dataset(n = 200, sim_seed = 123, problem = "meta-analysis")

Xlist = sim_data$Xlist; Ylist = sim_data$Ylist; Trtlist = sim_data$Trtlist

```

The main functions in this package are `mpersonalized`and `mpersonalized_cv`, where the second is the cross validation version of the first one. The first function will estimate a model for each given penalty parameter while the second will select the optimal one based on cross validation. 

To estimate the meta-analysis problem, we provide `Xlist`, `Ylist`, `Trtlist` to the functions `mpersonalized` and `mpersonalized_cv`, for instance, `mpersonalized(problem = "meta-analysis", Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist, ...)`. For the multiple outcomes, instead, we use code like `mpersonalized(problem = "multiple outcomes", X = X, Ylist = Ylist, Trt = Trt,...)`.

Errors will be reported if the arguments required are not provided:
```{r problem format, error = TRUE}
mpersonalized(problem = "meta-analysis",
              X = X, Ylist = Ylist, Trtlist = Trtlist)
mpersonalized(problem = "multiple outcomes",
              Xlist = Xlist, Ylist = Ylist, Trt = Trt)
```

`mpersonalized` function is based on the below framework:
$$\min_{g_1,\dots,g_K} \frac{1}{2}\sum_{k=1}^K \sum_{i=1}^{n_k}\frac{|\hat{C}_k(X_{ki})|}{\sum_{i=1}^{n_k}|\hat{C}_k(X_{ki})|}\bigl [1\{\hat{C}_k(X_{ki})>0\}-g_k(X_{ki})\bigr]^2 + h(g_1,\dots,g_K)$$
Here the regularization function $h$ is of the form of a sum of sparse group lasso and fused lasso penalty
$$h = (1-\alpha)\lambda_1\sqrt{q} \sum_{j=1}^p \|\boldsymbol{\beta_j}\|_2+\alpha \lambda_1  \sum_{j=1}^p \|\boldsymbol{\beta_j}\|_1+ \lambda_2 \sum_{j=1}^p \sum_{1\le a < b \le K}|\beta_{ja}-\beta_{jb}|$$
where $\boldsymbol{\beta_j}=(\beta_{j1},\dots,\beta_{jK})$.

The optimization depends on whether we have fused term. When $\lambda_2 = 0$, the model is implemented through `SGL' package, and when $\lambda_2 \ne 0$, the model is implemented through ADMM algorithm.

The function `mpersonalized` offers the following 8 different penalties for separate rules of different studies. They are: "linear", "lasso", "GL", "SGL", "fused", "lasso+fused", "GL+fused" and "SGL+fused".

There are two approaches if eventually we only want a single recommendation rule. 
1. We use the above framework to generate recommendations for each different study/outcome. Then the overall recommendation is made by weightedly sum the separate recommendations. The weight can be specified by user.
2. We directly estimate the overall rule recommendation by the following framework.
$$\min_{g_1,\dots,g_K} \frac{1}{2}\sum_{k=1}^K \sum_{i=1}^{n_k}\frac{|\hat{C}_k(X_{ki})|}{\sum_{i=1}^{n_k}|\hat{C}_k(X_{ki})|}\bigl [1\{\hat{C}_k(X_{ki})>0\}-g(X_{ki})\bigr]^2 + \lambda_{unique}\sum_{j=1}^p\|\beta_j\|_1$$
`mpersonalized` offers both approachs. The first approach can be carried in `prediction` function by specifying the weight we want. For the second approach,  we can sepcifiy `unique_rule = TRUE` in the usage of `mpersonalized` or `mpersonalized_cv` and specifying the penalty to be "linear" or "lasso". The function `mpersonalized_cv` does not include the "linear" model for both unique rule and separate rule setting.

No matter `unique_rule = TRUE` or `unique_rule = FALSE`, user should always specify a penalty first and then provide penalty parameters if needed. The default penalty parameters will also be computed automatically if not supplied. 

Next, we will use the "meta-analysis" problem as our example.

Based on the penalty user selected, the function will warning and automatically adpat if the supplied penalty parameter is not appropriate.
```{r test, warning = TRUE, error = TRUE}
model_SGL = mpersonalized(problem = "meta-analysis",
                          Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                          penalty = "SGL", lambda2 = seq(0.01, 0.1, 0.01), 
                          unique_rule = FALSE)
model_lasso = mpersonalized(problem = "meta-analysis",
                            Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                            penalty = "lasso", alpha = 0.5, unique_rule = FALSE)
```

The functions can still run but the warning message is good for better usage of the function next time.

The penalty parameters can be either specified by user or leave it NULL and use default sequence. If we want to use default penalty parameter, we can also set the length of penalty parameter sequence.
```{r deafult sequence}
model_lasso1 = mpersonalized(problem = "meta-analysis",
                            Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                            penalty = "lasso", unique_rule = FALSE)
model_lasso2 = mpersonalized(problem = "meta-analysis",
                            Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                            penalty = "lasso", lambda1 = seq(0.01, 0.1, 0.01), unique_rule = FALSE)
model_lasso3 = mpersonalized(problem = "meta-analysis",
                            Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                            penalty = "lasso", num_lambda1 = 10, unique_rule = FALSE)
model_lasso_fused = mpersonalized(problem = "meta-analysis",
                                  Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                                  penalty = "lasso+fused", num_lambda1 = 2, num_lambda2 = 2, 
                                  unique_rule = FALSE)

```

The penalty sequence can be withdrawn from the model. For each penalty parameter, a set of coefficients and intercept can be obtained. 
```{r result}
names(model_lasso1)

model_lasso1$lambda1 
model_lasso2$lambda2

names(model_lasso_fused)

model_lasso_fused$lambda1
model_lasso_fused$lambda2

#To obtain the beta and intercept for the 5th lambda1
beta = model_SGL$betalist[[5]]; intercept = model_SGL$intercept[[5]]
print(beta)
print(intercept)
```

We could also use cross validation to automatically choose the best tuning parameter. 

```{r cv method, error = TRUE, warning = TRUE}
#the default alpha in SGL is 0.95, but a smaller alpha works better in our case
model_SGL_cv = mpersonalized_cv(problem = "meta-analysis",
                                Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                                penalty = "SGL", cv_folds = 5, unique_rule = FALSE)
model_SGL_fused_cv = mpersonalized_cv(problem = "meta-analysis",
                                      Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                                      penalty = "SGL+fused", num_lambda1 = 2, num_lambda2 = 2,
                                      cv_folds = 3, unique_rule = FALSE)
```

Different from `mpersonalized`, the output from `mpersonalzied_cv` only has the optimal beta and intercept.
```{r cv result}
model_SGL_cv$lambda1
model_SGL_cv$lambda2

model_SGL_cv$opt_lambda1
model_SGL_cv$opt_lambda2

model_SGL_cv$beta
model_SGL_cv$intercept

model_SGL_fused_cv$lambda1
model_SGL_fused_cv$lambda2

model_SGL_fused_cv$opt_lambda1
model_SGL_fused_cv$opt_lambda2

model_SGL_fused_cv$beta
model_SGL_fused_cv$intercept
```

Besides, we could also try fit a single rule for all the studies.
```{r unique}
model_unique_lasso_cv = mpersonalized_cv(problem = "meta-analysis",
                                         Xlist = Xlist, Ylist = Ylist, Trtlist = Trtlist,
                                         penalty = "lasso", unique_rule = TRUE)

model_unique_lasso_cv$unique_rule_lambda
model_unique_lasso_cv$opt_unique_rule_lambda

model_unique_lasso_cv$beta
model_unique_lasso_cv$intercept
```

We could use `predict` function to predict the optimal treatment for new patients. We first generate some new patients. We could specify an overall weighted recommendation rule. The default weight is equal weight for each response/study. For object returned from `mpersonalized`  function, `predict` predicts the benefit score and treatment for each penalty parameter. If we does not supply `newx`, then `predict` predicts for the original data for each study based on its own rule.
```{r new patients}
p = model_SGL$number_covariates
newx = matrix(rnorm(5 * p), nrow = 5, ncol = p)
pred = predict(model_SGL, newx = newx, overall_rec = TRUE)

#for the 10th penalty parameter
pred$treatment[[10]]
pred$benefit_score[[10]]

pred_orig = predict(model_SGL)

#for the 10th penalty parameter
str(pred_orig$treatment[[10]])
str(pred_orig$benefit_score[[10]])
```

Similarly, `mpersonalized_cv` gives prediction for the optimal beta and intercept.
```{r new patients cv}
pred_cv = predict(model_SGL_cv, newx = newx, overall_rec = TRUE)
pred_cv$treatment
pred_cv$benefit_score

pred_orig_cv = predict(model_SGL_cv)

str(pred_orig_cv$treatment)
str(pred_orig_cv$benefit_score)
```

Last, we introduce `plot` method for `mpersonalize` and `mpersonalized_cv` output. The `plot` function draws the mean value of the response based on interaction levels of recommened treatment and received treatment.
```{r plot, fig.width = 8, fig.height = 4}
#to plot for output from mpersonalized, we need to supply the index of penalty parameter
#5th of lambda1, 1st of lambda2
plot = plot(model_SGL, ind1 = 5, ind2 = 1)
#for study 1
plot[[1]]
#for study 3
plot[[3]]

#plot for output from mpersonalized_cv does not need supply the index
plot = plot(model_SGL_cv)
#for study 1
plot[[1]]
#for study 3
plot[[3]]
```

This package is now available from Github at https://github.com/chenshengkuang/mpersonalized. 
